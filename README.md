# Deep-RL-for-Stock-Trading
The goal of this project is to see whether Deep reinforcement learning can be used to "beat" the stock market using basic data found on Yahoo Finance. While this is a problem the has not been solved by the most accomplished algorithmic traders using complex datasets, the average investor who does not have access to such data has limited pathways of making positive returns. If anything, making a model which can consistently deliver returns at or above the average market growth rate will be considered a successful implementation of a Deep RL approach.

# Data
The data used is sourced from Yahoo! Finance. Using the package "yfinance" in Python, the user is able to enter the symbol of the stock they'd like the information of and the data collection program will download a pandas dataframe which will be saved as a csv file. As this project is still in its early stages, a GUI has not yet been developed for the user to intuitively select their stock of interest, however, this will be developed once the program has reached its targeted performance metrics.

The training data is AAPL stock from the dates 01/01/2010 to 01/01/2018. The testing data will be AAPL stock from the dates 01/01/2018 to 01/01/2021.

# Methods
This project relies on Deep Q Learning, which is a model-free approach to learning the optimal trade strategy. The agent begins with a state, which is represented as the open price, the trading volume for the day, the month, the day, and the agent's current balance. The state is then passed through a neural network, which currently consists of 3 linear layers. The first hidden layer is of size 5, to distinguish simple patterns within data. The next hidden layer is of size 50, to account for more complex variations. The final layer is of size 3, to return the processed data into probabilities each action (buy, sell, or hold) occurring. From there, the agent generates a random proabability of exploring, and if it is less than or equal to the user's assigned exploration rate, the agent will choose a random action. Otherwise it will choose the action with the highest probability of occuring. 

Under this framework, the neural network acts as our Q-value function. The action chosen by the agent is then evaluated by a reward system, which is currently being adapted based on existing literature. The rudimentary idea in place at the moment describes the reward for buying or selling as the opportunity cost of holding until the end of the day. The agent in this environment can only act once a day at the opening price. As such, if they choose to buy at the opening bell but the price decreases by the end of the day, or if they sell at the opening price but the price increases by the end of the day, they will receive a negative reward. The inverse of these scenarios results in a positive reward.

After a reward has been assigned, the loss function is calculated, which is the mean squared error between the Q value returned by the network in the current state and the sum of the rewards plus the discounted future rewards. The discounted future rewards are calculated by taking the resulting state of the chosen action and using experience replay to return the maximal possible reward given that future state. Once the loss function is calculated, we perform back propogation on the network to optimize its parameter weights, ultimately having it converge to an optimal policy given an action. We feed each resulting state iteratively through this system until our program terminates, decaying our exploration rate by a user-defined fixed amount, and then repeat it for 500 episodes.

# Results
Conclusive results have not been determined yet as this model is still in its early stages of development. The current objectives for this project are to define a consistent and accurate reward metric, develop a more complex network that implements convolution, search for more publicly-accessible data, and test the effects of different state representations on final profit.

## Update 08/02/21
The rewards system was improved to accurately measure profit (this was done by calculating the difference between the value of 100 sold shares compared to their value upon purchase. The "sell" option was modified so the agent could only buy or sell 100 shares at a time). With a reliable rewards system, the simple DQN model could be trained on the training dataset. Graphs depicting the final account balance after training for 100 epochs and 500 epochs is included in the "charts" directory. 

After 100 epochs, the DQN seems to grow exponentially in performance, returning greater profits with each epoch, and even returning more than 7% annually for 8 years in some instances. This indicates that the model was learning well and that the training process was relatively stable.

After 500 epochs, we see much more variability in the model's performance. Until the (approximately) 275 epoch, the model continued to improve consistently and provide stable returns. However, from the 275 epoch and beyond there is increased variation in the distribution of final account balances. While the model still inidcates a general upwards trend, it shows signs of catestrophic forgetting, a situation in which the DQN forgets previously learned information. I believe that the DQN may be forgetting "failed" scenarios in which its performance was suboptimal due to its continuous period of success. As such, it executes unfavorable actions and has to relearn the optimal policy. Catestrophic forgetting is a common issue in reinforcement learning when the learning process is not as stable as initially thought, so improving model stability is my current goal. I will begin by exploring how I used experienced replay to train the model and see whether there were any issues with my approach or if there is room for improvement there.

## Update 08/08/21
After 100 epochs, a DDPG model performed roughly at the level of the standard DQN model after 275 epochs. This inidcates drastically improved learning performance, however, the DDPG model took significantly longer to train for 100 episodes. For final product, I will need to consider the optimal way to deploy the model so that it is both efficient and effective. Now I will train the DDPG model for 500 epochs, and still examine the catestrophic forgetting issue mentioned in the previous update.
