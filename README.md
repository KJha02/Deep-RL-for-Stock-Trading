# Deep-RL-for-Stock-Trading
The goal of this project is to see whether Deep reinforcement learning can be used to "beat" the stock market using basic data found on Yahoo Finance. While this is a problem the has not been solved by the most accomplished algorithmic traders using complex datasets, the average investor who does not have access to such data has limited pathways of making positive returns. If anything, making a model which can consistently deliver returns at or above the average market growth rate will be considered a successful implementation of a Deep RL approach.

# Data
The data used is sourced from Yahoo! Finance. Using the package "yfinance" in Python, the user is able to enter the symbol of the stock they'd like the information of and the data collection program will download a pandas dataframe which will be saved as a csv file. As this project is still in its early stages, a GUI has not yet been developed for the user to intuitively select their stock of interest, however, this will be developed once the program has reached its targeted performance metrics.

# Methods
This project relies on Deep Q Learning, which is a model-free approach to learning the optimal trade strategy. The agent begins with a state, which is represented as the open price, the trading volume for the day, the month, the day, and the agent's current balance. The state is then passed through a neural network, which currently consists of 3 linear layers. The first hidden layer is of size 5, to distinguish simple patterns within data. The next hidden layer is of size 50, to account for more complex variations. The final layer is of size 3, to return the processed data into probabilities each action (buy, sell, or hold) occurring. From there, the agent generates a random proabability of exploring, and if it is less than or equal to the user's assigned exploration rate, the agent will choose a random action. Otherwise it will choose the action with the highest probability of occuring. 

Under this framework, the neural network acts as our Q-value function. The action chosen by the agent is then evaluated by a reward system, which is currently being adapted based on existing literature. The rudimentary idea in place at the moment describes the reward for buying or selling as the opportunity cost of holding until the end of the day. The agent in this environment can only act once a day at the opening price. As such, if they choose to buy at the opening bell but the price decreases by the end of the day, or if they sell at the opening price but the price increases by the end of the day, they will receive a negative reward. The inverse of these scenarios results in a positive reward.

After a reward has been assigned, the loss function is calculated, which is the mean squared error between the Q value returned by the network in the current state and the sum of the rewards plus the discounted future rewards. The discounted future rewards are calculated by taking the resulting state of the chosen action and using experience replay to return the maximal possible reward given that future state. Once the loss function is calculated, we perform back propogation on the network to optimize its parameter weights, ultimately having it converge to an optimal policy given an action. We feed each resulting state iteratively through this system until our program terminates, decaying our exploration rate by a user-defined fixed amount, and then repeat it for 500 episodes.

# Results
Conclusive results have not been determined yet as this model is still in its early stages of development. The current objectives for this project are to define a consistent and accurate reward metric, develop a more complex network that implements convolution, search for more publicly-accessible data, and test the effects of different state representations on final profit.
